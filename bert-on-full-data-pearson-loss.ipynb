{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-08T20:48:08.248527Z",
     "iopub.status.busy": "2024-06-08T20:48:08.247878Z",
     "iopub.status.idle": "2024-06-08T20:48:23.044529Z",
     "shell.execute_reply": "2024-06-08T20:48:23.043554Z",
     "shell.execute_reply.started": "2024-06-08T20:48:08.248493Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install datasets --quiet\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T20:48:23.046689Z",
     "iopub.status.busy": "2024-06-08T20:48:23.046269Z",
     "iopub.status.idle": "2024-06-08T20:48:25.397714Z",
     "shell.execute_reply": "2024-06-08T20:48:25.396562Z",
     "shell.execute_reply.started": "2024-06-08T20:48:23.046661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-08 20:48:23--  https://raw.githubusercontent.com/neilrs123/Lexical-Complexity-Prediction/master/Dataset/Sub-task%201/lcp_single_train.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1746979 (1.7M) [text/plain]\n",
      "Saving to: 'train.tsv'\n",
      "\n",
      "train.tsv           100%[===================>]   1.67M  --.-KB/s    in 0.03s   \n",
      "\n",
      "2024-06-08 20:48:24 (64.0 MB/s) - 'train.tsv' saved [1746979/1746979]\n",
      "\n",
      "--2024-06-08 20:48:25--  https://raw.githubusercontent.com/neilrs123/Lexical-Complexity-Prediction/master/Dataset/Sub-task%201/lcp_single_trial.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 97235 (95K) [text/plain]\n",
      "Saving to: 'trial.tsv'\n",
      "\n",
      "trial.tsv           100%[===================>]  94.96K  --.-KB/s    in 0.009s  \n",
      "\n",
      "2024-06-08 20:48:25 (10.9 MB/s) - 'trial.tsv' saved [97235/97235]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O train.tsv \"https://raw.githubusercontent.com/neilrs123/Lexical-Complexity-Prediction/master/Dataset/Sub-task%201/lcp_single_train.tsv\"\n",
    "!wget -O trial.tsv \"https://raw.githubusercontent.com/neilrs123/Lexical-Complexity-Prediction/master/Dataset/Sub-task%201/lcp_single_trial.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T21:15:21.977116Z",
     "iopub.status.busy": "2024-06-08T21:15:21.976652Z",
     "iopub.status.idle": "2024-06-08T21:15:22.043468Z",
     "shell.execute_reply": "2024-06-08T21:15:22.042671Z",
     "shell.execute_reply.started": "2024-06-08T21:15:21.977062Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_table(\"/kaggle/working/train.tsv\")\n",
    "# train = train[train[\"corpus\"] == \"bible\"].drop([\"corpus\", \"id\"], axis = 1)\n",
    "\n",
    "trial = pd.read_table(\"/kaggle/working/trial.tsv\")\n",
    "# trial = trial[trial[\"subcorpus\"] == \"bible\"].drop([\"subcorpus\", \"id\"], axis = 1)\n",
    "\n",
    "train_data = train[:-100]\n",
    "trial = trial[:35].reset_index(drop = True)\n",
    "from datasets import Dataset\n",
    "\n",
    "train_data = Dataset.from_pandas(train_data)\n",
    "# eval_data = Dataset.from_pandas(eval_data)\n",
    "eval_data = Dataset.from_pandas(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T21:15:23.796133Z",
     "iopub.status.busy": "2024-06-08T21:15:23.795732Z",
     "iopub.status.idle": "2024-06-08T21:15:23.802609Z",
     "shell.execute_reply": "2024-06-08T21:15:23.801612Z",
     "shell.execute_reply.started": "2024-06-08T21:15:23.796099Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_quotations(text):\n",
    "    # Remove both double and single quotation marks\n",
    "    text = text.replace('\"', '')  # Remove double quotations\n",
    "    text = text.replace(\"'\", \"\")  # Remove single quotations\n",
    "    return text\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Apply remove_quotations function to both sentences and tokens\n",
    "    sentences = [remove_quotations(f\"{s} [SEP] {t}\") for s, t in zip(examples['sentence'], examples['token'])]\n",
    "    tokenized_inputs = tokenizer(sentences, padding=True, truncation=True, max_length=64)\n",
    "    tokenized_inputs['labels'] = examples['complexity']\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T21:15:26.060852Z",
     "iopub.status.busy": "2024-06-08T21:15:26.059853Z",
     "iopub.status.idle": "2024-06-08T21:15:27.440744Z",
     "shell.execute_reply": "2024-06-08T21:15:27.439803Z",
     "shell.execute_reply.started": "2024-06-08T21:15:26.060815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f44f97ccd046fc80ade3577468a47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7132 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46cc626a0e3c4bd5a5c433f413b5e277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "# Map the updated preprocess function to datasets\n",
    "train_dataset = train_data.map(preprocess_function, batched=True, batch_size=64)\n",
    "eval_dataset = eval_data.map(preprocess_function, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T21:15:08.626668Z",
     "iopub.status.busy": "2024-06-08T21:15:08.626323Z",
     "iopub.status.idle": "2024-06-08T21:15:10.580709Z",
     "shell.execute_reply": "2024-06-08T21:15:10.579934Z",
     "shell.execute_reply.started": "2024-06-08T21:15:08.626642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1654c07ee3434b07a8e3c7682a1e1a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model_path = 'bert-base-uncased'\n",
    "\n",
    "config = BertConfig.from_pretrained(model_path, num_labels=1, dropout=0.1, attention_dropout=0.1)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T21:01:53.483885Z",
     "iopub.status.busy": "2024-06-08T21:01:53.483483Z",
     "iopub.status.idle": "2024-06-08T21:01:53.488389Z",
     "shell.execute_reply": "2024-06-08T21:01:53.487352Z",
     "shell.execute_reply.started": "2024-06-08T21:01:53.483855Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "import torch\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T21:15:38.306623Z",
     "iopub.status.busy": "2024-06-08T21:15:38.305925Z",
     "iopub.status.idle": "2024-06-08T21:15:38.435085Z",
     "shell.execute_reply": "2024-06-08T21:15:38.434332Z",
     "shell.execute_reply.started": "2024-06-08T21:15:38.306592Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PearsonCorrelationLoss(nn.Module):\n",
    "    def forward(self, preds, labels):\n",
    "        preds = preds.squeeze()\n",
    "        labels = labels.squeeze()\n",
    "        \n",
    "        x_mean = torch.mean(preds)\n",
    "        y_mean = torch.mean(labels)\n",
    "        cov = torch.sum((preds - x_mean) * (labels - y_mean))\n",
    "        \n",
    "        x_var = torch.sum((preds - x_mean) ** 2)\n",
    "        y_var = torch.sum((labels - y_mean) ** 2)\n",
    "        \n",
    "        pearson_corr = cov / torch.sqrt(x_var * y_var)\n",
    "        return 1 - pearson_corr\n",
    "\n",
    "pearson_loss = PearsonCorrelationLoss()\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = pearson_loss(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T21:01:54.385689Z",
     "iopub.status.busy": "2024-06-08T21:01:54.384804Z",
     "iopub.status.idle": "2024-06-08T21:01:54.390702Z",
     "shell.execute_reply": "2024-06-08T21:01:54.389724Z",
     "shell.execute_reply.started": "2024-06-08T21:01:54.385651Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "def compute_metrics(pred):\n",
    "    preds = np.squeeze(pred.predictions)\n",
    "    return {\n",
    "            \"Pearson\" : pearsonr(preds,pred.label_ids)[0],\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T21:13:12.367705Z",
     "iopub.status.busy": "2024-06-08T21:13:12.366868Z",
     "iopub.status.idle": "2024-06-08T21:13:12.395218Z",
     "shell.execute_reply": "2024-06-08T21:13:12.394280Z",
     "shell.execute_reply.started": "2024-06-08T21:13:12.367673Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import torch\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/kaggle/working/model',\n",
    "    report_to=\"none\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='steps',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=20,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,  # Evaluate every 2000 steps\n",
    "    save_strategy=\"steps\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type='linear',  # Adjust learning rate scheduling\n",
    "    learning_rate=4e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T21:15:43.189547Z",
     "iopub.status.busy": "2024-06-08T21:15:43.188493Z",
     "iopub.status.idle": "2024-06-08T21:15:43.195339Z",
     "shell.execute_reply": "2024-06-08T21:15:43.194378Z",
     "shell.execute_reply.started": "2024-06-08T21:15:43.189504Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T21:15:45.955989Z",
     "iopub.status.busy": "2024-06-08T21:15:45.955325Z",
     "iopub.status.idle": "2024-06-08T21:20:56.648844Z",
     "shell.execute_reply": "2024-06-08T21:20:56.647909Z",
     "shell.execute_reply.started": "2024-06-08T21:15:45.955945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2676' max='2676' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2676/2676 05:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.918000</td>\n",
       "      <td>0.899605</td>\n",
       "      <td>0.074431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.661700</td>\n",
       "      <td>0.969921</td>\n",
       "      <td>0.172455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.497900</td>\n",
       "      <td>0.904321</td>\n",
       "      <td>0.193260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.363700</td>\n",
       "      <td>0.793247</td>\n",
       "      <td>0.294838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.385600</td>\n",
       "      <td>0.863126</td>\n",
       "      <td>0.399444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.383800</td>\n",
       "      <td>0.793843</td>\n",
       "      <td>0.344691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.362600</td>\n",
       "      <td>0.863155</td>\n",
       "      <td>0.290475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.414600</td>\n",
       "      <td>0.877314</td>\n",
       "      <td>0.314699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>0.839101</td>\n",
       "      <td>0.350112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.323200</td>\n",
       "      <td>0.987307</td>\n",
       "      <td>0.354541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.424100</td>\n",
       "      <td>0.980922</td>\n",
       "      <td>0.335851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.789241</td>\n",
       "      <td>0.386878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.364400</td>\n",
       "      <td>0.972044</td>\n",
       "      <td>0.305301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.363600</td>\n",
       "      <td>0.904996</td>\n",
       "      <td>0.312038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.370500</td>\n",
       "      <td>1.020207</td>\n",
       "      <td>0.385576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.346900</td>\n",
       "      <td>0.879424</td>\n",
       "      <td>0.399084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.363900</td>\n",
       "      <td>0.905161</td>\n",
       "      <td>0.304880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.299700</td>\n",
       "      <td>0.863280</td>\n",
       "      <td>0.401544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.231200</td>\n",
       "      <td>0.887039</td>\n",
       "      <td>0.346065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.327800</td>\n",
       "      <td>0.842961</td>\n",
       "      <td>0.299757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.298800</td>\n",
       "      <td>0.876758</td>\n",
       "      <td>0.321188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.191000</td>\n",
       "      <td>0.808618</td>\n",
       "      <td>0.346456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.244100</td>\n",
       "      <td>0.841912</td>\n",
       "      <td>0.328865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.240400</td>\n",
       "      <td>0.814802</td>\n",
       "      <td>0.382522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.201700</td>\n",
       "      <td>0.830686</td>\n",
       "      <td>0.374890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.157700</td>\n",
       "      <td>0.901438</td>\n",
       "      <td>0.327756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.252200</td>\n",
       "      <td>0.907104</td>\n",
       "      <td>0.311993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.229900</td>\n",
       "      <td>0.918625</td>\n",
       "      <td>0.310101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.224100</td>\n",
       "      <td>0.754234</td>\n",
       "      <td>0.243871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.186200</td>\n",
       "      <td>0.933569</td>\n",
       "      <td>0.311907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.358700</td>\n",
       "      <td>0.921050</td>\n",
       "      <td>0.308760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.258000</td>\n",
       "      <td>0.908504</td>\n",
       "      <td>0.310838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.223100</td>\n",
       "      <td>0.886382</td>\n",
       "      <td>0.331336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.213400</td>\n",
       "      <td>0.893870</td>\n",
       "      <td>0.378906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.210400</td>\n",
       "      <td>0.886100</td>\n",
       "      <td>0.344977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.919990</td>\n",
       "      <td>0.302954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.181200</td>\n",
       "      <td>0.892883</td>\n",
       "      <td>0.307327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.929797</td>\n",
       "      <td>0.286960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.921583</td>\n",
       "      <td>0.289316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.193900</td>\n",
       "      <td>0.903123</td>\n",
       "      <td>0.301211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.866333</td>\n",
       "      <td>0.330495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.147000</td>\n",
       "      <td>0.887494</td>\n",
       "      <td>0.343997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.209700</td>\n",
       "      <td>0.854149</td>\n",
       "      <td>0.328969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.188700</td>\n",
       "      <td>0.818251</td>\n",
       "      <td>0.348136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.195400</td>\n",
       "      <td>0.839981</td>\n",
       "      <td>0.358272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.119700</td>\n",
       "      <td>0.843213</td>\n",
       "      <td>0.376560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.186800</td>\n",
       "      <td>0.849571</td>\n",
       "      <td>0.379846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.139200</td>\n",
       "      <td>0.835472</td>\n",
       "      <td>0.398864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.103200</td>\n",
       "      <td>0.821083</td>\n",
       "      <td>0.389554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.275600</td>\n",
       "      <td>0.807877</td>\n",
       "      <td>0.386928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.159700</td>\n",
       "      <td>0.801212</td>\n",
       "      <td>0.385129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>0.807476</td>\n",
       "      <td>0.389178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.174600</td>\n",
       "      <td>0.814621</td>\n",
       "      <td>0.389929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# results of normal bert on full data\n",
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T21:01:56.718924Z",
     "iopub.status.busy": "2024-06-08T21:01:56.718567Z",
     "iopub.status.idle": "2024-06-08T21:02:34.046171Z",
     "shell.execute_reply": "2024-06-08T21:02:34.045195Z",
     "shell.execute_reply.started": "2024-06-08T21:01:56.718895Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='768' max='768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [768/768 00:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.032656</td>\n",
       "      <td>0.035876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.031900</td>\n",
       "      <td>0.026587</td>\n",
       "      <td>0.016872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.019078</td>\n",
       "      <td>0.149549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.009597</td>\n",
       "      <td>0.349048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.008611</td>\n",
       "      <td>0.358525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.015509</td>\n",
       "      <td>0.321659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.008346</td>\n",
       "      <td>0.266406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.009879</td>\n",
       "      <td>0.226679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.006963</td>\n",
       "      <td>0.172984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.015630</td>\n",
       "      <td>0.250749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.012232</td>\n",
       "      <td>0.224601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>0.010877</td>\n",
       "      <td>0.131474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.012274</td>\n",
       "      <td>0.204070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.010471</td>\n",
       "      <td>0.202581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.008910</td>\n",
       "      <td>0.213067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# results of bert-medium on bible data\n",
    "train_results = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
