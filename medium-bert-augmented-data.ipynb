{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, BertConfig, Trainer, TrainingArguments\nimport torch\nfrom scipy.stats import pearsonr\nimport numpy as np\nfrom transformers.models.bert.modeling_bert import BertModel\nfrom torch import nn\nfrom transformers.trainer_utils import set_seed\nimport random\nset_seed(42)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-08T23:58:57.607964Z","iopub.execute_input":"2024-06-08T23:58:57.608639Z","iopub.status.idle":"2024-06-08T23:59:16.017415Z","shell.execute_reply.started":"2024-06-08T23:58:57.608608Z","shell.execute_reply":"2024-06-08T23:59:16.016436Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-08 23:59:07.256100: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-08 23:59:07.256238: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-08 23:59:07.367696: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"!wget -O train.tsv \"https://raw.githubusercontent.com/neilrs123/Lexical-Complexity-Prediction/master/Dataset/Sub-task%201/lcp_single_train.tsv\"\n!wget -O trial.tsv \"https://raw.githubusercontent.com/neilrs123/Lexical-Complexity-Prediction/master/Dataset/Sub-task%201/lcp_single_trial.tsv\"","metadata":{"execution":{"iopub.status.busy":"2024-06-09T00:00:38.096145Z","iopub.execute_input":"2024-06-09T00:00:38.096968Z","iopub.status.idle":"2024-06-09T00:00:40.531734Z","shell.execute_reply.started":"2024-06-09T00:00:38.096938Z","shell.execute_reply":"2024-06-09T00:00:40.530749Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"--2024-06-09 00:00:38--  https://raw.githubusercontent.com/neilrs123/Lexical-Complexity-Prediction/master/Dataset/Sub-task%201/lcp_single_train.tsv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1746979 (1.7M) [text/plain]\nSaving to: 'train.tsv'\n\ntrain.tsv           100%[===================>]   1.67M  --.-KB/s    in 0.04s   \n\n2024-06-09 00:00:39 (45.1 MB/s) - 'train.tsv' saved [1746979/1746979]\n\n--2024-06-09 00:00:40--  https://raw.githubusercontent.com/neilrs123/Lexical-Complexity-Prediction/master/Dataset/Sub-task%201/lcp_single_trial.tsv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 97235 (95K) [text/plain]\nSaving to: 'trial.tsv'\n\ntrial.tsv           100%[===================>]  94.96K  --.-KB/s    in 0.01s   \n\n2024-06-09 00:00:40 (7.39 MB/s) - 'trial.tsv' saved [97235/97235]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load data\ntrain = pd.read_table(\"/kaggle/working/train.tsv\")\ntrial = pd.read_table(\"/kaggle/working/trial.tsv\")\n\n# Split data for training and evaluation\ntrain_data = train[:-100]\neval_data = trial[:35].reset_index(drop=True)\n\ndef data_augmentation(df):\n    augmented_sentences = []\n    augmented_tokens = []\n    augmented_complexity = []\n    for _, row in df.iterrows():\n        sentence = row['sentence']\n        token = row['token']\n        complexity = row['complexity']\n        words = sentence.split()\n        if len(words) > 1:\n            # Random Insertion\n            if random.random() < 0.2:\n                words.insert(random.randint(0, len(words) - 1), random.choice(words))\n\n            # Random Swap\n            if random.random() < 0.2:\n                i, j = random.sample(range(len(words)), 2)\n                words[i], words[j] = words[j], words[i]\n            # Random Deletion\n            if random.random() < 0.2:\n                del words[random.randint(0, len(words) - 1)]\n\n        augmented_sentences.append(' '.join(words))\n        augmented_tokens.append(token)\n        augmented_complexity.append(complexity)\n\n    augmented_df = pd.DataFrame({'sentence': augmented_sentences, 'token': augmented_tokens, 'complexity': augmented_complexity})\n    return pd.concat([df, augmented_df], ignore_index=True)\n\n# Apply data augmentation to train_data\ntrain_data = data_augmentation(train_data)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T00:38:09.883684Z","iopub.execute_input":"2024-06-09T00:38:09.884577Z","iopub.status.idle":"2024-06-09T00:38:10.454017Z","shell.execute_reply.started":"2024-06-09T00:38:09.884541Z","shell.execute_reply":"2024-06-09T00:38:10.453152Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"train_data.drop([\"id\", \"corpus\"], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T00:38:10.734971Z","iopub.execute_input":"2024-06-09T00:38:10.735682Z","iopub.status.idle":"2024-06-09T00:38:10.742797Z","shell.execute_reply.started":"2024-06-09T00:38:10.735644Z","shell.execute_reply":"2024-06-09T00:38:10.741670Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2024-06-09T00:38:11.795961Z","iopub.execute_input":"2024-06-09T00:38:11.796882Z","iopub.status.idle":"2024-06-09T00:38:11.809308Z","shell.execute_reply.started":"2024-06-09T00:38:11.796850Z","shell.execute_reply":"2024-06-09T00:38:11.808261Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"                                                sentence          token  \\\n0      Behold, there came up out of the river seven c...          river   \n1      I am a fellow bondservant with you and with yo...       brothers   \n2      The man, the lord of the land, said to us, 'By...       brothers   \n3      Shimei had sixteen sons and six daughters; but...       brothers   \n4      He has put my brothers far from me.\\tbrothers\\...           sons   \n...                                                  ...            ...   \n14259  The transatlantic partnership ceased to be mer...  transatlantic   \n14260  â€“ Mr President, ladies and gentlemen, I take t...    trepidation   \n14261  ERDF, ESF and Cohesion Fund: provisions relati...       Cohesion   \n14262  General provisions on the European Regional De...       Cohesion   \n14263  I believe, however, that there has been a cert...         rigour   \n\n       complexity  \n0        0.000000  \n1        0.000000  \n2        0.050000  \n3        0.150000  \n4        0.055556  \n...           ...  \n14259    0.470588  \n14260    0.470588  \n14261    0.470588  \n14262    0.500000  \n14263    0.470588  \n\n[14264 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>token</th>\n      <th>complexity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Behold, there came up out of the river seven c...</td>\n      <td>river</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I am a fellow bondservant with you and with yo...</td>\n      <td>brothers</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The man, the lord of the land, said to us, 'By...</td>\n      <td>brothers</td>\n      <td>0.050000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Shimei had sixteen sons and six daughters; but...</td>\n      <td>brothers</td>\n      <td>0.150000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>He has put my brothers far from me.\\tbrothers\\...</td>\n      <td>sons</td>\n      <td>0.055556</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14259</th>\n      <td>The transatlantic partnership ceased to be mer...</td>\n      <td>transatlantic</td>\n      <td>0.470588</td>\n    </tr>\n    <tr>\n      <th>14260</th>\n      <td>â€“ Mr President, ladies and gentlemen, I take t...</td>\n      <td>trepidation</td>\n      <td>0.470588</td>\n    </tr>\n    <tr>\n      <th>14261</th>\n      <td>ERDF, ESF and Cohesion Fund: provisions relati...</td>\n      <td>Cohesion</td>\n      <td>0.470588</td>\n    </tr>\n    <tr>\n      <th>14262</th>\n      <td>General provisions on the European Regional De...</td>\n      <td>Cohesion</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>14263</th>\n      <td>I believe, however, that there has been a cert...</td>\n      <td>rigour</td>\n      <td>0.470588</td>\n    </tr>\n  </tbody>\n</table>\n<p>14264 rows Ã— 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Create datasets from pandas DataFrames\ntrain_dataset = Dataset.from_pandas(train_data)\neval_dataset = Dataset.from_pandas(eval_data)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T00:38:14.280521Z","iopub.execute_input":"2024-06-09T00:38:14.281193Z","iopub.status.idle":"2024-06-09T00:38:14.315413Z","shell.execute_reply.started":"2024-06-09T00:38:14.281158Z","shell.execute_reply":"2024-06-09T00:38:14.314547Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"# Preprocessing function\ndef remove_quotations(text):\n    text = text.replace('\"', '')\n    text = text.replace(\"'\", \"\")\n    return text\n\ndef preprocess_function(examples):\n    sentences = [remove_quotations(f\"[CLS] {s} [SEP] {t}\") for s, t in zip(examples['sentence'], examples['token'])] \n    tokenized_inputs = tokenizer(sentences, padding=True, truncation=True, max_length=64)\n    tokenized_inputs['labels'] = examples['complexity']\n    return tokenized_inputs\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2024-06-09T00:38:31.008632Z","iopub.execute_input":"2024-06-09T00:38:31.009330Z","iopub.status.idle":"2024-06-09T00:38:31.225515Z","shell.execute_reply.started":"2024-06-09T00:38:31.009297Z","shell.execute_reply":"2024-06-09T00:38:31.224515Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# Apply preprocessing to datasets\ntrain_dataset = train_dataset.map(preprocess_function, batched=True, batch_size=64)\neval_dataset = eval_dataset.map(preprocess_function, batched=True, batch_size=16)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T00:38:33.874203Z","iopub.execute_input":"2024-06-09T00:38:33.874680Z","iopub.status.idle":"2024-06-09T00:38:36.208920Z","shell.execute_reply.started":"2024-06-09T00:38:33.874644Z","shell.execute_reply":"2024-06-09T00:38:36.207965Z"},"trusted":true},"execution_count":82,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14264 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7f409a0b79b412dbf2c46c1866aabab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/35 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eae310500fe74d639f45e92a802dd6f3"}},"metadata":{}}]},{"cell_type":"code","source":"model_path = \"prajjwal1/bert-medium\"\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\nconfig = BertConfig.from_pretrained(model_path, num_labels=1, dropout=0.1, attention_dropout=0.1)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path, config = config)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T00:48:23.605684Z","iopub.execute_input":"2024-06-09T00:48:23.606084Z","iopub.status.idle":"2024-06-09T00:48:24.026917Z","shell.execute_reply.started":"2024-06-09T00:48:23.606043Z","shell.execute_reply":"2024-06-09T00:48:24.026182Z"},"trusted":true},"execution_count":117,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-medium and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-06-09T00:48:28.245729Z","iopub.execute_input":"2024-06-09T00:48:28.246692Z","iopub.status.idle":"2024-06-09T00:48:28.253966Z","shell.execute_reply.started":"2024-06-09T00:48:28.246658Z","shell.execute_reply":"2024-06-09T00:48:28.252974Z"},"trusted":true},"execution_count":118,"outputs":[{"execution_count":118,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n      (position_embeddings): Embedding(512, 512)\n      (token_type_embeddings): Embedding(2, 512)\n      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-7): 8 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=512, out_features=512, bias=True)\n              (key): Linear(in_features=512, out_features=512, bias=True)\n              (value): Linear(in_features=512, out_features=512, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=512, out_features=512, bias=True)\n              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=512, out_features=2048, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=2048, out_features=512, bias=True)\n            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=512, out_features=512, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=512, out_features=1, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/model',\n    report_to=\"none\",\n    do_train=True,\n    do_eval=True,\n    num_train_epochs=3,\n    warmup_steps=100,\n    weight_decay=0.01,\n    logging_strategy='steps',\n    logging_dir='./logs',\n    logging_steps=20,\n    evaluation_strategy=\"steps\",\n    eval_steps=50,\n    save_strategy=\"steps\",\n    fp16=torch.cuda.is_available(),\n    load_best_model_at_end=True,\n    lr_scheduler_type='linear', \n    learning_rate=4e-5,\n)\n\n# Compute metrics function\ndef compute_metrics(pred):\n    preds = np.squeeze(pred.predictions)\n    return {\"Pearson\": pearsonr(preds, pred.label_ids)[0]}\n\n\n# Create the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T01:06:54.548473Z","iopub.execute_input":"2024-06-09T01:06:54.549296Z","iopub.status.idle":"2024-06-09T01:06:54.586996Z","shell.execute_reply.started":"2024-06-09T01:06:54.549260Z","shell.execute_reply":"2024-06-09T01:06:54.586007Z"},"trusted":true},"execution_count":125,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch import cuda\nimport torch\ndevice = 'cuda' if cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-06-09T00:48:41.220058Z","iopub.execute_input":"2024-06-09T00:48:41.220882Z","iopub.status.idle":"2024-06-09T00:48:41.225806Z","shell.execute_reply.started":"2024-06-09T00:48:41.220849Z","shell.execute_reply":"2024-06-09T00:48:41.224152Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T00:48:42.409121Z","iopub.execute_input":"2024-06-09T00:48:42.409507Z","iopub.status.idle":"2024-06-09T00:52:57.697413Z","shell.execute_reply.started":"2024-06-09T00:48:42.409474Z","shell.execute_reply":"2024-06-09T00:52:57.696412Z"},"trusted":true},"execution_count":121,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5349' max='5349' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5349/5349 04:14, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Pearson</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.034400</td>\n      <td>0.045976</td>\n      <td>0.191736</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.028200</td>\n      <td>0.044446</td>\n      <td>0.262092</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.022100</td>\n      <td>0.005429</td>\n      <td>0.493086</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.023800</td>\n      <td>0.039771</td>\n      <td>0.175657</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.022200</td>\n      <td>0.031562</td>\n      <td>0.292797</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.017800</td>\n      <td>0.039137</td>\n      <td>0.355760</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.029300</td>\n      <td>0.006736</td>\n      <td>0.302324</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.023900</td>\n      <td>0.012337</td>\n      <td>0.271580</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.014700</td>\n      <td>0.020559</td>\n      <td>0.246089</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.016700</td>\n      <td>0.007242</td>\n      <td>0.101000</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.018100</td>\n      <td>0.006764</td>\n      <td>0.219854</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.011700</td>\n      <td>0.014930</td>\n      <td>0.328204</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.014100</td>\n      <td>0.019618</td>\n      <td>0.352291</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.013600</td>\n      <td>0.006661</td>\n      <td>0.267847</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.012100</td>\n      <td>0.017563</td>\n      <td>0.355958</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.015800</td>\n      <td>0.006793</td>\n      <td>0.202039</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.009900</td>\n      <td>0.011871</td>\n      <td>0.230301</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.013300</td>\n      <td>0.006159</td>\n      <td>0.229227</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.009900</td>\n      <td>0.007813</td>\n      <td>0.360947</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.012300</td>\n      <td>0.005717</td>\n      <td>0.342523</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.010900</td>\n      <td>0.009230</td>\n      <td>0.368391</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.014100</td>\n      <td>0.011776</td>\n      <td>0.388816</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.010000</td>\n      <td>0.010910</td>\n      <td>0.378749</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.009700</td>\n      <td>0.009608</td>\n      <td>0.305438</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.013200</td>\n      <td>0.009555</td>\n      <td>0.209735</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.012500</td>\n      <td>0.014910</td>\n      <td>0.189895</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.011200</td>\n      <td>0.010644</td>\n      <td>0.233932</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.011500</td>\n      <td>0.009255</td>\n      <td>0.234561</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.013400</td>\n      <td>0.007101</td>\n      <td>0.244622</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.010600</td>\n      <td>0.016347</td>\n      <td>0.284372</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.009000</td>\n      <td>0.013137</td>\n      <td>0.295913</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.008600</td>\n      <td>0.008396</td>\n      <td>0.337882</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.009400</td>\n      <td>0.005238</td>\n      <td>0.354245</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.008600</td>\n      <td>0.008776</td>\n      <td>0.297592</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.007500</td>\n      <td>0.005968</td>\n      <td>0.246926</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.005900</td>\n      <td>0.008511</td>\n      <td>0.273785</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.009200</td>\n      <td>0.007681</td>\n      <td>0.269846</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.007700</td>\n      <td>0.010798</td>\n      <td>0.274241</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.007000</td>\n      <td>0.012475</td>\n      <td>0.239982</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.006300</td>\n      <td>0.010633</td>\n      <td>0.208836</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.006500</td>\n      <td>0.010301</td>\n      <td>0.246280</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.006700</td>\n      <td>0.012419</td>\n      <td>0.244150</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.007800</td>\n      <td>0.008896</td>\n      <td>0.186621</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.007500</td>\n      <td>0.010288</td>\n      <td>0.170414</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.005100</td>\n      <td>0.009922</td>\n      <td>0.228750</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.006300</td>\n      <td>0.015433</td>\n      <td>0.225167</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.007200</td>\n      <td>0.007550</td>\n      <td>0.286113</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.005800</td>\n      <td>0.007757</td>\n      <td>0.245967</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.007300</td>\n      <td>0.008925</td>\n      <td>0.273692</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.006200</td>\n      <td>0.010506</td>\n      <td>0.282539</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>0.005500</td>\n      <td>0.006274</td>\n      <td>0.304266</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.006000</td>\n      <td>0.010172</td>\n      <td>0.271971</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>0.006600</td>\n      <td>0.006536</td>\n      <td>0.244340</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.008000</td>\n      <td>0.008002</td>\n      <td>0.165131</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>0.006900</td>\n      <td>0.008412</td>\n      <td>0.194505</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.006000</td>\n      <td>0.007263</td>\n      <td>0.207481</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>0.004700</td>\n      <td>0.009873</td>\n      <td>0.259344</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.005900</td>\n      <td>0.009167</td>\n      <td>0.223908</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>0.006600</td>\n      <td>0.009348</td>\n      <td>0.169455</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.006400</td>\n      <td>0.010096</td>\n      <td>0.219998</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>0.004300</td>\n      <td>0.013519</td>\n      <td>0.178096</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.007800</td>\n      <td>0.006313</td>\n      <td>0.197132</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>0.004700</td>\n      <td>0.009576</td>\n      <td>0.158072</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.005100</td>\n      <td>0.011198</td>\n      <td>0.160206</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>0.007000</td>\n      <td>0.009401</td>\n      <td>0.185440</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.007000</td>\n      <td>0.010955</td>\n      <td>0.155179</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>0.007200</td>\n      <td>0.012746</td>\n      <td>0.125312</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.006400</td>\n      <td>0.009409</td>\n      <td>0.126812</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>0.006200</td>\n      <td>0.011321</td>\n      <td>0.149163</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.006300</td>\n      <td>0.016599</td>\n      <td>0.129453</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>0.005300</td>\n      <td>0.009512</td>\n      <td>0.144379</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.005200</td>\n      <td>0.007300</td>\n      <td>0.161736</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>0.005400</td>\n      <td>0.008265</td>\n      <td>0.137593</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.003700</td>\n      <td>0.007431</td>\n      <td>0.141767</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>0.005500</td>\n      <td>0.008815</td>\n      <td>0.158886</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.003400</td>\n      <td>0.012142</td>\n      <td>0.136856</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>0.005200</td>\n      <td>0.011630</td>\n      <td>0.096175</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.004400</td>\n      <td>0.010396</td>\n      <td>0.097663</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>0.004200</td>\n      <td>0.009261</td>\n      <td>0.149055</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.003100</td>\n      <td>0.010323</td>\n      <td>0.172965</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>0.004200</td>\n      <td>0.010109</td>\n      <td>0.171084</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.003200</td>\n      <td>0.010164</td>\n      <td>0.145707</td>\n    </tr>\n    <tr>\n      <td>4150</td>\n      <td>0.004100</td>\n      <td>0.012980</td>\n      <td>0.123687</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.004100</td>\n      <td>0.011615</td>\n      <td>0.131965</td>\n    </tr>\n    <tr>\n      <td>4250</td>\n      <td>0.003800</td>\n      <td>0.010183</td>\n      <td>0.123949</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.003000</td>\n      <td>0.008546</td>\n      <td>0.134277</td>\n    </tr>\n    <tr>\n      <td>4350</td>\n      <td>0.005800</td>\n      <td>0.008989</td>\n      <td>0.125603</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.003100</td>\n      <td>0.008359</td>\n      <td>0.143311</td>\n    </tr>\n    <tr>\n      <td>4450</td>\n      <td>0.004800</td>\n      <td>0.010466</td>\n      <td>0.125829</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.004600</td>\n      <td>0.008660</td>\n      <td>0.116145</td>\n    </tr>\n    <tr>\n      <td>4550</td>\n      <td>0.003600</td>\n      <td>0.010807</td>\n      <td>0.106461</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.004800</td>\n      <td>0.009220</td>\n      <td>0.148660</td>\n    </tr>\n    <tr>\n      <td>4650</td>\n      <td>0.004200</td>\n      <td>0.008523</td>\n      <td>0.143104</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.005000</td>\n      <td>0.008682</td>\n      <td>0.152534</td>\n    </tr>\n    <tr>\n      <td>4750</td>\n      <td>0.004400</td>\n      <td>0.008491</td>\n      <td>0.152779</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.004700</td>\n      <td>0.010626</td>\n      <td>0.133085</td>\n    </tr>\n    <tr>\n      <td>4850</td>\n      <td>0.004800</td>\n      <td>0.010428</td>\n      <td>0.126718</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.005800</td>\n      <td>0.010569</td>\n      <td>0.123660</td>\n    </tr>\n    <tr>\n      <td>4950</td>\n      <td>0.004100</td>\n      <td>0.009175</td>\n      <td>0.127008</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.004400</td>\n      <td>0.009246</td>\n      <td>0.137765</td>\n    </tr>\n    <tr>\n      <td>5050</td>\n      <td>0.003700</td>\n      <td>0.008998</td>\n      <td>0.126935</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.003800</td>\n      <td>0.008881</td>\n      <td>0.131326</td>\n    </tr>\n    <tr>\n      <td>5150</td>\n      <td>0.004000</td>\n      <td>0.009613</td>\n      <td>0.118502</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.003400</td>\n      <td>0.009072</td>\n      <td>0.118248</td>\n    </tr>\n    <tr>\n      <td>5250</td>\n      <td>0.003800</td>\n      <td>0.009059</td>\n      <td>0.123851</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.003200</td>\n      <td>0.009127</td>\n      <td>0.124273</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":121,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5349, training_loss=0.008452752126273897, metrics={'train_runtime': 254.4202, 'train_samples_per_second': 168.194, 'train_steps_per_second': 21.024, 'total_flos': 418744272337920.0, 'train_loss': 0.008452752126273897, 'epoch': 3.0})"},"metadata":{}}]}]}