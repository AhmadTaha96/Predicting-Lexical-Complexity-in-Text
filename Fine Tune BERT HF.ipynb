{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-07T01:38:24.314064Z",
     "iopub.status.busy": "2024-06-07T01:38:24.313679Z",
     "iopub.status.idle": "2024-06-07T01:38:35.047731Z",
     "shell.execute_reply": "2024-06-07T01:38:35.046831Z",
     "shell.execute_reply.started": "2024-06-07T01:38:24.314033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65e421c4a1b4a6284c923c5a45f520e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a808f3ebb5432e96d83f16c1422a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2875c9930d4144f2ad304abaf5b89d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ae512c3b4d46afb6fd5d78ee00b0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe950229aef4e55a5e241cff5c426a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertTokenizerFast, BertForSequenceClassification\n",
    "\n",
    "model_path = 'bert-base-uncased'\n",
    "config = BertConfig.from_pretrained(model_path, num_labels=1, dropout=0.3, attention_dropout=0.3)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T01:07:46.541939Z",
     "iopub.status.busy": "2024-06-07T01:07:46.540721Z",
     "iopub.status.idle": "2024-06-07T01:07:49.854659Z",
     "shell.execute_reply": "2024-06-07T01:07:49.853566Z",
     "shell.execute_reply.started": "2024-06-07T01:07:46.541889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-07 01:07:47--  https://raw.githubusercontent.com/neilrs123/Lexical-Complexity-Prediction/master/Dataset/Sub-task%201/lcp_single_train.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1746979 (1.7M) [text/plain]\n",
      "Saving to: 'train.tsv'\n",
      "\n",
      "train.tsv           100%[===================>]   1.67M  7.08MB/s    in 0.2s    \n",
      "\n",
      "2024-06-07 01:07:48 (7.08 MB/s) - 'train.tsv' saved [1746979/1746979]\n",
      "\n",
      "--2024-06-07 01:07:49--  https://raw.githubusercontent.com/neilrs123/Lexical-Complexity-Prediction/master/Dataset/Sub-task%201/lcp_single_trial.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 97235 (95K) [text/plain]\n",
      "Saving to: 'trial.tsv'\n",
      "\n",
      "trial.tsv           100%[===================>]  94.96K  --.-KB/s    in 0.08s   \n",
      "\n",
      "2024-06-07 01:07:49 (1.10 MB/s) - 'trial.tsv' saved [97235/97235]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O train.tsv \"https://raw.githubusercontent.com/neilrs123/Lexical-Complexity-Prediction/master/Dataset/Sub-task%201/lcp_single_train.tsv\"\n",
    "!wget -O trial.tsv \"https://raw.githubusercontent.com/neilrs123/Lexical-Complexity-Prediction/master/Dataset/Sub-task%201/lcp_single_trial.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T01:43:32.111867Z",
     "iopub.status.busy": "2024-06-07T01:43:32.111498Z",
     "iopub.status.idle": "2024-06-07T01:43:32.150638Z",
     "shell.execute_reply": "2024-06-07T01:43:32.149776Z",
     "shell.execute_reply.started": "2024-06-07T01:43:32.111836Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_table(\"/kaggle/working/train.tsv\")\n",
    "# train = train[train[\"corpus\"] == \"bible\"].drop([\"corpus\", \"id\"], axis = 1)\n",
    "\n",
    "trial = pd.read_table(\"/kaggle/working/trial.tsv\")\n",
    "# trial = trial[trial[\"subcorpus\"] == \"bible\"].drop([\"subcorpus\", \"id\"], axis = 1)\n",
    "\n",
    "train_data = train[:-100]\n",
    "trial = trial[:35].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T01:43:33.868445Z",
     "iopub.status.busy": "2024-06-07T01:43:33.868081Z",
     "iopub.status.idle": "2024-06-07T01:43:33.901910Z",
     "shell.execute_reply": "2024-06-07T01:43:33.901136Z",
     "shell.execute_reply.started": "2024-06-07T01:43:33.868416Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_data = Dataset.from_pandas(train_data)\n",
    "# eval_data = Dataset.from_pandas(eval_data)\n",
    "eval_data = Dataset.from_pandas(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T01:59:58.165207Z",
     "iopub.status.busy": "2024-06-07T01:59:58.164209Z",
     "iopub.status.idle": "2024-06-07T01:59:58.188112Z",
     "shell.execute_reply": "2024-06-07T01:59:58.187130Z",
     "shell.execute_reply.started": "2024-06-07T01:59:58.165163Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T02:00:01.778769Z",
     "iopub.status.busy": "2024-06-07T02:00:01.778393Z",
     "iopub.status.idle": "2024-06-07T02:00:03.433222Z",
     "shell.execute_reply": "2024-06-07T02:00:03.432186Z",
     "shell.execute_reply.started": "2024-06-07T02:00:01.778739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa4bbe7c0e74aa0b19c0707490152db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7132 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d248588cc07d4e41b56010d9cb83c604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocess function: sentence + [SEP] + token\n",
    "def preprocess_function(examples):\n",
    "    sentences = [f\"{s} [SEP] {t}\" for s, t in zip(examples['sentence'], examples['token'])]\n",
    "    tokenized_inputs = tokenizer(sentences, padding=True, truncation=True, max_length = 256)\n",
    "    tokenized_inputs['labels'] = examples['complexity']\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Map the preprocess function to datasets\n",
    "train_dataset = train_data.map(preprocess_function, batched=True, batch_size=512)\n",
    "eval_dataset = eval_data.map(preprocess_function, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T01:35:54.942495Z",
     "iopub.status.busy": "2024-06-07T01:35:54.942156Z",
     "iopub.status.idle": "2024-06-07T01:35:54.947098Z",
     "shell.execute_reply": "2024-06-07T01:35:54.946075Z",
     "shell.execute_reply.started": "2024-06-07T01:35:54.942470Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "import torch\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T01:46:00.944296Z",
     "iopub.status.busy": "2024-06-07T01:46:00.943897Z",
     "iopub.status.idle": "2024-06-07T01:46:00.972068Z",
     "shell.execute_reply": "2024-06-07T01:46:00.971041Z",
     "shell.execute_reply.started": "2024-06-07T01:46:00.944266Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/kaggle/working/model',\n",
    "    report_to=\"none\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    num_train_epochs=10,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='steps',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type='linear',  # Adjust learning rate scheduling\n",
    "    learning_rate=2e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T01:46:05.152953Z",
     "iopub.status.busy": "2024-06-07T01:46:05.152184Z",
     "iopub.status.idle": "2024-06-07T01:46:05.157818Z",
     "shell.execute_reply": "2024-06-07T01:46:05.156881Z",
     "shell.execute_reply.started": "2024-06-07T01:46:05.152910Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "def compute_metrics(pred):\n",
    "    preds = np.squeeze(pred.predictions)\n",
    "    return {\n",
    "            \"Pearson\" : pearsonr(preds,pred.label_ids)[0],\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T01:46:06.256506Z",
     "iopub.status.busy": "2024-06-07T01:46:06.255446Z",
     "iopub.status.idle": "2024-06-07T01:46:06.270533Z",
     "shell.execute_reply": "2024-06-07T01:46:06.269581Z",
     "shell.execute_reply.started": "2024-06-07T01:46:06.256456Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T01:46:06.689046Z",
     "iopub.status.busy": "2024-06-07T01:46:06.688669Z",
     "iopub.status.idle": "2024-06-07T01:46:06.693323Z",
     "shell.execute_reply": "2024-06-07T01:46:06.692391Z",
     "shell.execute_reply.started": "2024-06-07T01:46:06.689017Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T01:46:07.912219Z",
     "iopub.status.busy": "2024-06-07T01:46:07.911302Z",
     "iopub.status.idle": "2024-06-07T01:58:16.386234Z",
     "shell.execute_reply": "2024-06-07T01:58:16.384959Z",
     "shell.execute_reply.started": "2024-06-07T01:46:07.912187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3184' max='8920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3184/8920 12:07 < 21:50, 4.38 it/s, Epoch 3.57/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.009698</td>\n",
       "      <td>0.063624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>0.015114</td>\n",
       "      <td>0.196144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.012409</td>\n",
       "      <td>0.074176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.006446</td>\n",
       "      <td>0.155445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.006362</td>\n",
       "      <td>0.186522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.007277</td>\n",
       "      <td>0.310145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.005780</td>\n",
       "      <td>0.218614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.015901</td>\n",
       "      <td>0.200371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.008906</td>\n",
       "      <td>0.120852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.012018</td>\n",
       "      <td>0.109669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.015074</td>\n",
       "      <td>0.190427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.008081</td>\n",
       "      <td>0.192019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.008186</td>\n",
       "      <td>0.219645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.011492</td>\n",
       "      <td>0.195593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.013009</td>\n",
       "      <td>0.230743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.007632</td>\n",
       "      <td>0.286918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.009669</td>\n",
       "      <td>0.208505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.013851</td>\n",
       "      <td>0.241844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>0.214524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.009611</td>\n",
       "      <td>0.216476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.007391</td>\n",
       "      <td>0.195152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.007655</td>\n",
       "      <td>0.204211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.010573</td>\n",
       "      <td>0.239178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.007512</td>\n",
       "      <td>0.229852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.007154</td>\n",
       "      <td>0.135664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.008636</td>\n",
       "      <td>0.099170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.010468</td>\n",
       "      <td>0.104475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.008398</td>\n",
       "      <td>0.089011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.011615</td>\n",
       "      <td>0.071834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.007398</td>\n",
       "      <td>0.158875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.010089</td>\n",
       "      <td>0.188674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.010271</td>\n",
       "      <td>0.162031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.177385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.006699</td>\n",
       "      <td>0.160919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.007197</td>\n",
       "      <td>0.145212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.009538</td>\n",
       "      <td>0.039970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.010624</td>\n",
       "      <td>0.106196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.012627</td>\n",
       "      <td>0.055389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.009157</td>\n",
       "      <td>0.029855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.008016</td>\n",
       "      <td>-0.072160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.007240</td>\n",
       "      <td>-0.093637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.010383</td>\n",
       "      <td>-0.021537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.008145</td>\n",
       "      <td>0.038398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.006713</td>\n",
       "      <td>0.151406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.006306</td>\n",
       "      <td>0.100543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.095559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.006941</td>\n",
       "      <td>0.009864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.006316</td>\n",
       "      <td>0.106227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.006468</td>\n",
       "      <td>0.138196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>0.146369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.009293</td>\n",
       "      <td>0.171769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.006617</td>\n",
       "      <td>0.162622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.007340</td>\n",
       "      <td>0.129158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.012188</td>\n",
       "      <td>0.140963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.007456</td>\n",
       "      <td>0.068514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.006945</td>\n",
       "      <td>0.035649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.007287</td>\n",
       "      <td>0.084367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>0.126998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.006687</td>\n",
       "      <td>0.096488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.006758</td>\n",
       "      <td>0.158724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.010255</td>\n",
       "      <td>0.122069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.007358</td>\n",
       "      <td>0.101058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.007881</td>\n",
       "      <td>0.019024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2221\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
