{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-20T09:37:46.623110Z",
     "iopub.status.busy": "2024-06-20T09:37:46.622769Z",
     "iopub.status.idle": "2024-06-20T09:37:50.498873Z",
     "shell.execute_reply": "2024-06-20T09:37:50.498035Z",
     "shell.execute_reply.started": "2024-06-20T09:37:46.623081Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:37:50.501221Z",
     "iopub.status.busy": "2024-06-20T09:37:50.500804Z",
     "iopub.status.idle": "2024-06-20T09:37:50.655919Z",
     "shell.execute_reply": "2024-06-20T09:37:50.654903Z",
     "shell.execute_reply.started": "2024-06-20T09:37:50.501180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:37:50.658026Z",
     "iopub.status.busy": "2024-06-20T09:37:50.657618Z",
     "iopub.status.idle": "2024-06-20T09:39:45.860586Z",
     "shell.execute_reply": "2024-06-20T09:39:45.859665Z",
     "shell.execute_reply.started": "2024-06-20T09:37:50.657994Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 09:37:52.246041: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-20 09:37:52.246176: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-20 09:37:52.362443: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3768ed54eff04096b4c783310870c72a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:39:45.863963Z",
     "iopub.status.busy": "2024-06-20T09:39:45.862885Z",
     "iopub.status.idle": "2024-06-20T09:39:45.871893Z",
     "shell.execute_reply": "2024-06-20T09:39:45.870936Z",
     "shell.execute_reply.started": "2024-06-20T09:39:45.863935Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_model(\n",
    "        system_message,\n",
    "        user_message,\n",
    "        temperature=0.1,\n",
    "        max_length=50,\n",
    "        top_p = 0.9\n",
    "        ):\n",
    "    start_time = time()\n",
    "    user_message = \"Question: \" + user_message + \" Answer:\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "        ]\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "        )\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    sequences = pipeline(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        top_p=top_p,\n",
    "        temperature=temperature,\n",
    "        #num_return_sequences=1,\n",
    "        eos_token_id=terminators,\n",
    "        max_new_tokens=max_length,\n",
    "        return_full_text=False,\n",
    "        pad_token_id=pipeline.model.config.eos_token_id\n",
    "    )\n",
    "    #answer = f\"{sequences[0]['generated_text'][len(prompt):]}\\n\"\n",
    "    answer = sequences[0]['generated_text']\n",
    "    end_time = time()\n",
    "    ttime = f\"Total time: {round(end_time-start_time, 2)} sec.\"\n",
    "\n",
    "    return user_message + \" \" + answer  + \" \" +  ttime\n",
    "\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are an AI assistant designed to answer simple questions.\n",
    "Please restrict your answer to the exact question asked.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:39:45.873207Z",
     "iopub.status.busy": "2024-06-20T09:39:45.872949Z",
     "iopub.status.idle": "2024-06-20T09:39:45.883074Z",
     "shell.execute_reply": "2024-06-20T09:39:45.882298Z",
     "shell.execute_reply.started": "2024-06-20T09:39:45.873170Z"
    }
   },
   "outputs": [],
   "source": [
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Reasoning\", \"Question\", \"Answer\", \"Total time\"], [\"blue\", \"red\", \"green\", \"magenta\"]):\n",
    "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:01:58.277942Z",
     "iopub.status.busy": "2024-06-18T18:01:58.277208Z",
     "iopub.status.idle": "2024-06-18T18:02:29.446139Z",
     "shell.execute_reply": "2024-06-18T18:02:29.445231Z",
     "shell.execute_reply.started": "2024-06-18T18:01:58.277908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Can you tell if word in sentence is lexically complex or no? I will give you sentence and you predict the lexical complexity between 0 and 1, outputting inly one number, can you do it? \n",
       "\n",
       "**<font color='green'>Answer:</font>** Yes, I can predict the lexical complexity of a word in a sentence. Please provide the sentence, and I'll output a number between 0 and 1, where 0 represents a simple word and 1 represents a complex word. \n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 31.16 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_model(\n",
    "    system_message,\n",
    "    user_message=\"Can you tell if word in sentence is lexically complex or no? I will give you sentence and you predict the lexical complexity between 0 and 1, outputting inly one number, can you do it?\",\n",
    "    temperature=0.1,\n",
    "    max_length=256)\n",
    "display(Markdown(colorize_text(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:26:23.254121Z",
     "iopub.status.busy": "2024-06-18T23:26:23.253749Z",
     "iopub.status.idle": "2024-06-18T23:26:26.009141Z",
     "shell.execute_reply": "2024-06-18T23:26:26.007728Z",
     "shell.execute_reply.started": "2024-06-18T23:26:23.254069Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-18 23:26:24--  https://raw.githubusercontent.com/neilrs123/Lexical-Complexity-Prediction/master/Dataset/Sub-task%201/lcp_single_train.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1746979 (1.7M) [text/plain]\n",
      "Saving to: 'train.tsv'\n",
      "\n",
      "train.tsv           100%[===================>]   1.67M  --.-KB/s    in 0.02s   \n",
      "\n",
      "2024-06-18 23:26:24 (71.8 MB/s) - 'train.tsv' saved [1746979/1746979]\n",
      "\n",
      "--2024-06-18 23:26:25--  https://raw.githubusercontent.com/neilrs123/Lexical-Complexity-Prediction/master/Dataset/Sub-task%201/lcp_single_trial.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 97235 (95K) [text/plain]\n",
      "Saving to: 'trial.tsv'\n",
      "\n",
      "trial.tsv           100%[===================>]  94.96K  --.-KB/s    in 0.008s  \n",
      "\n",
      "2024-06-18 23:26:25 (11.1 MB/s) - 'trial.tsv' saved [97235/97235]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O train.tsv \"https://raw.githubusercontent.com/neilrs123/Lexical-Complexity-Prediction/master/Dataset/Sub-task%201/lcp_single_train.tsv\"\n",
    "\n",
    "!wget -O trial.tsv \"https://raw.githubusercontent.com/neilrs123/Lexical-Complexity-Prediction/master/Dataset/Sub-task%201/lcp_single_trial.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:26:38.496547Z",
     "iopub.status.busy": "2024-06-18T23:26:38.496199Z",
     "iopub.status.idle": "2024-06-18T23:26:38.538731Z",
     "shell.execute_reply": "2024-06-18T23:26:38.537789Z",
     "shell.execute_reply.started": "2024-06-18T23:26:38.496523Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train, trial = df = pd.read_table('train.tsv'), pd.read_table('trial.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:26:39.404717Z",
     "iopub.status.busy": "2024-06-18T23:26:39.404059Z",
     "iopub.status.idle": "2024-06-18T23:26:39.414806Z",
     "shell.execute_reply": "2024-06-18T23:26:39.413736Z",
     "shell.execute_reply.started": "2024-06-18T23:26:39.404685Z"
    }
   },
   "outputs": [],
   "source": [
    "train = train[train[\"corpus\"] == \"bible\"].drop([\"corpus\", \"id\"], axis = 1)\n",
    "trial = trial[trial[\"subcorpus\"] == \"bible\"].drop([\"subcorpus\", \"id\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:55:17.125987Z",
     "iopub.status.busy": "2024-06-18T23:55:17.125580Z",
     "iopub.status.idle": "2024-06-18T23:55:17.141811Z",
     "shell.execute_reply": "2024-06-18T23:55:17.140873Z",
     "shell.execute_reply.started": "2024-06-18T23:55:17.125935Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "data = Dataset.from_pandas(trial.iloc[:35])\n",
    "from transformers import set_seed\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "set_seed(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama: temp: 0.5, best prompt ---> 34%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T00:09:12.003495Z",
     "iopub.status.busy": "2024-06-19T00:09:12.003120Z",
     "iopub.status.idle": "2024-06-19T00:11:05.122683Z",
     "shell.execute_reply": "2024-06-19T00:11:05.121663Z",
     "shell.execute_reply.started": "2024-06-19T00:09:12.003465Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [01:53<00:00,  3.23s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for row in tqdm(data):\n",
    "    token, sentence = row[\"token\"], row[\"sentence\"]\n",
    "    \n",
    "    response = query_model(\n",
    "    system_message,\n",
    "    user_message=f\"\"\"Estimate the lexical complexity of the token in the context, give one single float value between 0 and 1\n",
    "            where 0 means the lowest lexical complexity and 1 means the highest lexical complexity (very complex).\n",
    "            the third quartile at 0.371 meaning most of the tokens complexity below 0.371.\n",
    "            use all possible features to accuratly estimate it:\n",
    "            The Context: {row['sentence']}\n",
    "            Token: {row['token']}\"\"\",\n",
    "    temperature = 0.5,\n",
    "    max_length=25)\n",
    "#     display(Markdown(colorize_text(response)))\n",
    "    match = re.search(pattern, response)\n",
    "    if match:\n",
    "        number = float(match.group(1))\n",
    "    else:\n",
    "        number = None\n",
    "    results.append(number)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T00:11:19.906296Z",
     "iopub.status.busy": "2024-06-19T00:11:19.905554Z",
     "iopub.status.idle": "2024-06-19T00:11:19.916327Z",
     "shell.execute_reply": "2024-06-19T00:11:19.915101Z",
     "shell.execute_reply.started": "2024-06-19T00:11:19.906258Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.22951723108012925, pvalue=0.18473522973917983)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr(results, data[\"complexity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T00:00:01.080882Z",
     "iopub.status.busy": "2024-06-19T00:00:01.079932Z",
     "iopub.status.idle": "2024-06-19T00:00:01.087373Z",
     "shell.execute_reply": "2024-06-19T00:00:01.086421Z",
     "shell.execute_reply.started": "2024-06-19T00:00:01.080838Z"
    }
   },
   "outputs": [],
   "source": [
    "    user_message=f\"\"\"Estimate the lexical complexity of the token in the context, give one single float value between 0 and 1\n",
    "            the third quartile at 0.371 meaning most of the tokens complexity below 0.371.\n",
    "            use all possible features to accuratly estimate it:\n",
    "            The Context: {row['sentence']}\n",
    "            Token: {row['token']}\"\"\",\n",
    "    temperature = 0.5,\n",
    "    max_length=25)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best result 34%"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
